<div align="center">

# ğŸ¦  ZoonoMoE

### *Frictionless zoonotic surveillance, routed at the edge.*

> **Speak a field report. Get a veterinary risk assessment spoken back â€” fully on-device, in under 20 seconds.**

<br/>

[![Python](https://img.shields.io/badge/Python-3.11+-3776AB?style=for-the-badge&logo=python&logoColor=white)](https://python.org)
[![MLX](https://img.shields.io/badge/Apple_MLX-Accelerated-FF6B35?style=for-the-badge&logo=apple&logoColor=white)](https://ml-explore.github.io/mlx/)
[![Flask](https://img.shields.io/badge/Flask-3.x-000000?style=for-the-badge&logo=flask&logoColor=white)](https://flask.palletsprojects.com/)
[![Whisper](https://img.shields.io/badge/Whisper-ASR-412991?style=for-the-badge&logo=openai&logoColor=white)](https://github.com/openai/whisper)
[![License](https://img.shields.io/badge/License-MIT-A6E3A1?style=for-the-badge)](LICENSE)

<br/>

```
 User says â†’ "Three chickens died overnight, cyanotic combs, one found convulsing."
                              â†“  ~18 seconds later
 ZoonoMoE â†’ "RISK LEVEL: HIGH â€” consistent with HPAI. Isolate birds. Report to DLD."
```

</div>

---

## âš¡ Pipeline at a Glance

```
ğŸ¤ Voice / âŒ¨ï¸  Text
        â”‚
        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  [1] ASR     â”‚ â”€â”€â–¶â”‚  [2] NER     â”‚ â”€â”€â–¶â”‚  [3] Router  â”‚ â”€â”€â–¶â”‚  [4] RAG     â”‚ â”€â”€â–¶â”‚  [5] Expert  â”‚ â”€â”€â–¶â”‚  [6] TTS     â”‚
â”‚  MLX Whisper â”‚    â”‚  Qwen3 JSON  â”‚    â”‚  MiniLM+MLP  â”‚    â”‚  FAISS/domainâ”‚    â”‚  Qwen3-4B    â”‚    â”‚  Kokoro-82M  â”‚
â”‚  + halluc.   â”‚    â”‚  extraction  â”‚    â”‚  6 domains   â”‚    â”‚  top-3 chunksâ”‚    â”‚  streaming   â”‚    â”‚  streaming   â”‚
â”‚  guard       â”‚    â”‚              â”‚    â”‚  + off-topic â”‚    â”‚              â”‚    â”‚  risk card   â”‚    â”‚  audio       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**All processing is on-device â€” no cloud API, no data leaves your machine.**

---

## ğŸš€ Quickstart

**Requirements:** macOS + Apple Silicon (M1â€“M4) Â· Python 3.11+ Â· `ffmpeg` in PATH

```bash
# Clone and install
git clone https://github.com/E27-25/MLOps_Project.git
cd MLOps_Project
python3 -m venv venv && source venv/bin/activate
pip install -r requirements.txt

# Run
python3 app.py
```

Open **http://localhost:7860** â€” landing page â†’ click **Launch App** â†’ record or type a field report.

```bash
# Optional: reduce memory usage with a smaller Whisper model
WHISPER_SIZE=tiny python3 app.py
```

---

## ğŸ§¬ Step-by-Step Workflow

<details>
<summary><b>Step 1 â€” ASR Â· MLX Whisper</b></summary>
<br/>

| | |
|---|---|
| **Model** | `mlx-community/whisper-base-mlx` |
| **Input** | WebM blob from browser `MediaRecorder` |
| **Convert** | `ffmpeg` â†’ 16 kHz mono WAV |
| **Guard** | Single word repeating >6Ã— or top-3 words covering >80% â†’ rejected, user prompted to re-record |

```python
result = mlx_whisper.transcribe(wav_path, path_or_hf_repo="mlx-community/whisper-base-mlx")
transcript = result["text"].strip()
```

</details>

<details>
<summary><b>Step 2 â€” NER Â· Qwen3 JSON Extraction</b></summary>
<br/>

Structured JSON-mode prompt extracts 8 fields from the raw transcript:

| Field | Example output |
|---|---|
| `species` | `["chicken", "duck"]` |
| `symptoms` | `["cyanotic combs", "twisted neck"]` |
| `mortality_count` | `30` |
| `affected_count` | `50` |
| `location` | `"near the pond"` |
| `timeframe` | `"this morning"` |
| `reporter_role` | `"farmer"` |
| `raw_summary` | Plain one-liner for downstream prompts |

</details>

<details>
<summary><b>Step 3 â€” MoE Router Â· MiniLM + MLP</b></summary>
<br/>

**Architecture:** `all-MiniLM-L6-v2` sentence embeddings â†’ `MLPClassifier(128, 64)` â†’ 6 disease domains

**Off-topic guard:** Regex checks for greetings/chat before the MLP runs. If matched with no mortality signal â†’ routes to `general` instantly.

| Domain | Training samples |
|---|---|
| `avian_flu` | 27 |
| `fmd` | 25 |
| `general` | 25 |
| `leptospirosis` | 26 |
| `nipah_hendra` | 30 |
| `rabies` | 27 |

**Cross-val F1 (macro, 5-fold): `0.740`**

To retrain after editing `data/router_training.jsonl`:
```bash
python3 -c "
from pathlib import Path
from models.router import train
train(model_dir=Path('models'), extra_data=Path('data/router_training.jsonl'))
"
```

</details>

<details>
<summary><b>Step 4 â€” RAG Â· Per-domain FAISS</b></summary>
<br/>

- One vector index per disease domain (`knowledge_base/{domain}/index.pkl`)
- Embedder: `sentence-transformers/all-MiniLM-L6-v2`
- Top-3 chunks retrieved by cosine similarity
- Fallback: built-in seed knowledge if index fails to load (no crash)
- Retrieved chunks shown in the **RAG Sources** collapsible panel

</details>

<details>
<summary><b>Step 5 â€” LLM Expert Â· Qwen3-4B streaming</b></summary>
<br/>

- Model: `mlx-community/Qwen3-4B-4bit`
- Domain-specific expert persona prompt + NER fields + RAG context
- `<think>...</think>` blocks stripped transparently from the SSE stream
- Output includes: live-streamed text, `RISK LEVEL` badge, `report_to_authorities` flag
- Markdown rendered client-side (`**bold**`, `*italic*`, bullet lists)

</details>

<details>
<summary><b>Step 6 â€” TTS Â· Kokoro-82M streaming</b></summary>
<br/>

- Model: `hexgrad/Kokoro-82M`, voice `af_heart`
- Each complete sentence â†’ WAV chunk â†’ base64 SSE event â†’ `AudioContext` queue
- **No wait** for full response â€” first audio plays ~3â€“5s after LLM starts
- Controls: â¸ Pause Â· â–¶ Resume Â· â¹ Stop

</details>

---

## ğŸ§ª Test Inputs

Use these in the text field to verify domain routing:

| Domain | Input |
|---|---|
| ğŸ¦ Avian Flu | `30 chickens died this morning with purple combs and twisted necks, labored breathing` |
| ğŸ„ FMD | `My cattle have blisters on tongue and feet, limping badly and salivating heavily` |
| ğŸ– Nipah/Hendra | `Pig farmer here â€” pigs died suddenly overnight, two workers now have fever and confusion` |
| ğŸ€ Leptospirosis | `Five rice farmers have fever, muscle pain, red eyes after wading in flooded paddy fields` |
| ğŸ• Rabies | `A stray dog bit two children â€” foaming at the mouth, running in circles before collapsing` |
| ğŸ’¬ Chat | `Hi, how do I protect myself when working near livestock?` |

---

## ğŸ—‚ï¸ Project Structure

```
MLOps_Project/
â”œâ”€â”€ app.py                        # Flask app â€” full 6-stage pipeline
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ setup.sh                      # First-run setup
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ docker-compose.yml
â”‚
â”œâ”€â”€ data/
â”‚   â””â”€â”€ router_training.jsonl     # 100 training examples (6 domains)
â”‚
â”œâ”€â”€ knowledge_base/
â”‚   â”œâ”€â”€ avian_flu/                # FAISS index + raw veterinary docs
â”‚   â”œâ”€â”€ fmd/
â”‚   â”œâ”€â”€ nipah_hendra/
â”‚   â”œâ”€â”€ rabies/
â”‚   â”œâ”€â”€ leptospirosis/
â”‚   â””â”€â”€ general/
â”‚
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ router.pkl                # Trained MLP classifier
â”‚   â”œâ”€â”€ router_meta.json
â”‚   â””â”€â”€ router.py                 # Training + inference code
â”‚
â”œâ”€â”€ static/
â”‚   â”œâ”€â”€ css/style.css             # Catppuccin Mocha theme
â”‚   â”œâ”€â”€ js/app.js                 # Frontend pipeline controller
â”‚   â””â”€â”€ favicon.svg
â”‚
â”œâ”€â”€ templates/
â”‚   â”œâ”€â”€ landing.html              # Landing page (orbital particle canvas)
â”‚   â””â”€â”€ index.html                # Main app UI
â”‚
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ discord_logger.py         # Forum-mode pipeline + retrain logger
â”‚   â””â”€â”€ evaluate.py               # Router evaluation script
â”‚
â””â”€â”€ utils/
    â””â”€â”€ rag.py                    # RAG retrieval + compat unpickler
```

---

## âš™ï¸ Configuration

| Variable | Default | Description |
|---|---|---|
| `USE_MLX` | `1` | MLX Whisper (`1`) or OpenAI Whisper (`0`) |
| `LLM_MODEL` | `mlx-community/Qwen3-4B-4bit` | LLM repo path |
| `WHISPER_SIZE` | `base` | `tiny` Â· `base` Â· `small` Â· `medium` |
| `PORT` | `7860` | Server port |
| `DEBUG` | `0` | Flask debug mode |
| `DISCORD_WEBHOOK` | _(empty)_ | Discord Forum webhook URL for pipeline logging |

---

## ğŸ³ Docker Deployment

> For cloud / Linux servers. Mac M-series users should run natively â€” Docker won't use MLX/GPU.

```bash
# Build and start
docker compose up --build

# With Discord logging enabled
DISCORD_WEBHOOK="https://discord.com/api/webhooks/..." docker compose up --build
```

Persisted volumes (survive container restarts):

| Volume | Purpose |
|---|---|
| `./knowledge_base` | RAG FAISS indexes |
| `./models` | Trained MLP router |
| `./data` | Router training data |
| `./scripts` | Logger + evaluation scripts |

The container automatically health-checks `GET /health` every 30 s with a 60 s grace period for model loading.

---

## ğŸ“¡ Discord Pipeline Logging

Every inference run can be logged to a **Discord Forum channel** as its own thread.

```bash
pip install "discordflow[system]"
export DISCORD_WEBHOOK="https://discord.com/api/webhooks/YOUR_ID/YOUR_TOKEN"
python3 app.py
```

Each Forum thread contains:

| Stage | Logged data |
|---|---|
| ASR | Backend, model size, latency, transcript attachment |
| NER | All 8 extracted fields |
| Router | Domain, confidence, all 6 domain scores |
| RAG | Chunks count, latency, `rag_sources.txt` attachment |
| LLM | Risk level, report flag, `llm_assessment.txt` attachment |
| TTS | Audio chunks, latency |
| Summary | Total latency + CPU/RAM system metrics |

Test without a real webhook (dry-run prints to stdout):

```bash
python3 scripts/discord_logger.py
```

---

## ğŸ“Š Performance

| Metric | Value |
|---|---|
| Router F1 (macro, 6-class) | **0.740** |
| ASR latency | **~1â€“2 s** (Whisper base, Apple M-series) |
| Full pipeline | **~15â€“20 s** end-to-end |
| TTS first chunk | **~3â€“5 s** after LLM starts |

---

## âš ï¸ Known Issues

| Issue | Cause | Fix |
|---|---|---|
| Segfault (exit 139) | MLX + PyTorch competing for unified memory | Use `WHISPER_SIZE=tiny` or upgrade to â‰¥32 GB RAM |
| Whisper hallucination | Noisy/silent audio causes token loop | Hallucination guard rejects + prompts re-record |
| Router misclassification on edge cases | Overlapping symptom profiles | Retrain with expanded `router_training.jsonl` |

---

## ğŸ› ï¸ Technology Stack

| Layer | Technology |
|---|---|
| ASR | [MLX Whisper](https://github.com/ml-exploration/mlx-examples) |
| LLM / NER | [Qwen3-4B-4bit](https://huggingface.co/mlx-community/Qwen3-4B-4bit) via MLX |
| Router | `all-MiniLM-L6-v2` + scikit-learn MLP |
| RAG | FAISS + `sentence-transformers` |
| TTS | [Kokoro-82M](https://github.com/hexgrad/kokoro) |
| Backend | Flask 3.x + SSE streaming |
| Frontend | Vanilla JS Â· Canvas API Â· Web Audio API |

---

<div align="center">

*Built for MLOps coursework Â· KMITL Â· 2026*

</div>
